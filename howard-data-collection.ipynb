{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c89fc0-a6ae-4330-914e-edfbdf11f792",
   "metadata": {},
   "source": [
    "## SoccerCovid - Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef7c69d-dccc-41ae-af85-5e1c997d617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e707dc-83fd-4e71-b3f1-8eeaab909728",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_match_raw(url):\n",
    "    return BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "def get_parced_data(raw):\n",
    "    return BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "def get_match_schedule(raw):\n",
    "    if raw.find_all(\"time\"):\n",
    "        return raw.find_all(\"time\")[0].string\n",
    "\n",
    "def get_venue_name(raw):\n",
    "    if raw.find_all(\"span\", {\"class\": \"sdc-site-match-header__detail-venue\"}):\n",
    "        return raw.find_all(\"span\", {\"class\": \"sdc-site-match-header__detail-venue\"})[0].string\n",
    "\n",
    "def get_match_title(raw):\n",
    "    return raw.title.string\n",
    "\n",
    "def get_team_names(title, title_regex):\n",
    "    title_group = title_regex.match(title)\n",
    "    if title_group:\n",
    "        return title_group.group('teamA'), title_group.group('teamB')\n",
    "\n",
    "def get_team_scores(title, title_regex):\n",
    "    title_group = title_regex.match(title)\n",
    "    if title_group:\n",
    "        return title_group.group('teamA_score'), title_group.group('teamB_score')\n",
    "\n",
    "def get_winner_details(record):\n",
    "    if(record['teamA_score'] and record['teamA_score'] > record['teamB_score']):\n",
    "        return record['teamA'], record['teamA_score']\n",
    "    elif(record['teamA_score']):\n",
    "        return record['teamB'], record['teamB_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2822735-6de0-4694-bade-23cdd9149bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.skysports.com/premier-league-results/'\n",
    "\n",
    "def get_match_details(year_range, file_name):\n",
    "    print('Entered get_match_details')\n",
    "    if not os.path.exists('match_details_'+file_name):\n",
    "        if not os.path.exists('raw_'+file_name):\n",
    "            site = str(requests.get(base_url + year_range).content) #Fetch HTML Page\n",
    "            match_url_regex = r'<a href=\"(https:\\/\\/www\\.skysports\\.com\\/football\\/[a-zA-Z-\\/]+\\d+)\" class=\"matches__item matches__link\"'\n",
    "            match_url_regex = re.compile(match_url_regex)\n",
    "            match_urls = match_url_regex.findall(site)\n",
    "            match_details = pd.DataFrame(match_urls, columns =['url'])    \n",
    "            match_details['raw'] = match_details['url'].apply(get_match_raw)\n",
    "            match_details.to_csv('raw_'+file_name)\n",
    "        else:\n",
    "            match_details = pd.read_csv('raw_'+file_name)\n",
    "            match_details['raw'] = match_details['raw'].apply(get_parced_data)\n",
    "\n",
    "            match_details['schedule'] = match_details['raw'].apply(get_match_schedule)\n",
    "            match_details['venue'] = match_details['raw'].apply(get_venue_name)    \n",
    "            match_details['raw_title'] = match_details['raw'].apply(get_match_title)\n",
    "            title_regex = re.compile(r'(?P<teamA>[a-zA-Z_ \\']*) (?P<teamA_score>\\d?\\d) - (?P<teamB_score>\\d?\\d) (?P<teamB>[a-zA-Z_ \\']*) -')\n",
    "            match_details[['teamA', 'teamB']] = pd.DataFrame(match_details['raw_title']\n",
    "                                                             .apply(lambda x: get_team_names(x, title_regex))\n",
    "                                                             .tolist(), index=match_details.index)\n",
    "            match_details[['teamA_score', 'teamB_score']] = pd.DataFrame(match_details['raw_title']\n",
    "                                                                         .apply(lambda x: get_team_scores(x, title_regex))\n",
    "                                                                         .tolist(), index=match_details.index)\n",
    "            match_details[['winner_Team','winner_score']] = pd.DataFrame(match_details\n",
    "                                                                         .apply(get_winner_details, axis=1)\n",
    "                                                                         .tolist(), index=match_details.index)  \n",
    "\n",
    "            match_details.drop(columns=['raw','raw_title'], inplace=True)\n",
    "            match_details.to_csv('match_details_'+file_name)\n",
    "            \n",
    "            print('Completed get_match_details')\n",
    "            return match_details\n",
    "    else:\n",
    "        print('Completed get_match_details')\n",
    "        return pd.read_csv('match_details_'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e233db6-63ae-4ba8-94e9-8e7c68a6a953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chromedriver_path = 'C:/webdrivers/chromedriver'\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# Link config\n",
    "google_base_url = 'https://www.google.com/search?q='\n",
    "\n",
    "class SoccerCovid:\n",
    "    def __init__(self):\n",
    "        self.final_data = {}\n",
    "        pass\n",
    "\n",
    "    def get_driver(self):\n",
    "        return webdriver.Chrome(service=service)\n",
    "    \n",
    "    def get_url(self, param):\n",
    "        param += ' wikipedia'\n",
    "        param = param.replace(\" \", \"+\") \n",
    "        return ''.join([google_base_url, param])\n",
    "    \n",
    "    def get_by_xpath(self, driver, xpath):\n",
    "        return driver.find_elements(By.XPATH, xpath)\n",
    "        \n",
    "    def navigate_to_site(self, driver, url):\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def get_wiki_url(self, driver, param, ext = False):\n",
    "        param = str(param)\n",
    "        param += ' Football Club' if ext else ''\n",
    "        url = self.get_url(param)\n",
    "        self.navigate_to_site(driver, url)\n",
    "        xpath = \"//a[contains(@href, 'wikipedia.org/wiki')]\"\n",
    "        url_list = driver.find_elements(By.XPATH, xpath)\n",
    "        return url_list[0].get_attribute('href')\n",
    "    \n",
    "    def get_venue_location(self, match, driver):\n",
    "        wiki_url = self.get_wiki_url(driver, match['venue'])\n",
    "        self.navigate_to_site(driver, wiki_url)  \n",
    "        xpath = \"//th[contains(text(), 'Location' )]/following-sibling::td\"\n",
    "        raw_venue_location = self.get_by_xpath(driver, xpath)\n",
    "        if raw_venue_location:\n",
    "            return raw_venue_location[0].text\n",
    "    \n",
    "    def get_winner_location(self, match, driver):\n",
    "        wiki_url = self.get_wiki_url(driver, match['winner_Team'], True)\n",
    "        self.navigate_to_site(driver, wiki_url)\n",
    "        xpath = \"//th[contains(text(), 'Ground' )]/following-sibling::td/a\"\n",
    "        raw_venue_url = self.get_by_xpath(driver, xpath)\n",
    "        if raw_venue_url:\n",
    "            venue_url = raw_venue_url[0].get_attribute('href')\n",
    "            self.navigate_to_site(driver, venue_url)\n",
    "            xpath = \"//th[contains(text(), 'Location' )]/following-sibling::td\"\n",
    "            raw_winner_loaction = self.get_by_xpath(driver, xpath)\n",
    "            if raw_winner_loaction:\n",
    "                return raw_winner_loaction[0].text\n",
    "    \n",
    "    def get_unique_venues(self, match_details, driver):\n",
    "        unique_venue = pd.DataFrame()\n",
    "        unique_venue['venue'] = match_details['venue'].unique()\n",
    "        unique_venue['venue_location'] = unique_venue.apply(lambda x: self.get_venue_location(x, driver), axis = 1)\n",
    "        return unique_venue\n",
    "    \n",
    "    def get_unique_winners(self, match_details, driver):\n",
    "        unique_winner = pd.DataFrame()\n",
    "        unique_winner['winner_Team'] = match_details['winner_Team'].unique()\n",
    "        unique_winner['winner_location'] = unique_winner.apply(lambda x: self.get_winner_location(x, driver), axis = 1)\n",
    "        return unique_winner\n",
    "        \n",
    "    def scrape_pages(self, match_details):\n",
    "        driver = self.get_driver()\n",
    "        \n",
    "        unique_venue = self.get_unique_venues(match_details, driver)\n",
    "        match_details = match_details.merge(unique_venue, on='venue', how='left')\n",
    "        \n",
    "        unique_winner = self.get_unique_winners(match_details, driver)\n",
    "        match_details = match_details.merge(unique_winner, on='winner_Team', how='left')\n",
    "        \n",
    "        driver.quit()\n",
    "        return match_details\n",
    "    \n",
    "    def start_scraping(self, match_details_df):\n",
    "        return self.scrape_pages(match_details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede3231-6914-42e1-baab-be6b62707efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2019-20', '2020-21', '2021-22']\n",
    "\n",
    "for date in dates:\n",
    "    file_name = 'covid_soccer_' + date + '.csv'\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f'Started {file_name}')\n",
    "        result_df = get_match_details(date, file_name)\n",
    "        result_df = SoccerCovid().start_scraping(result_df)\n",
    "        result_df.to_csv(file_name)\n",
    "        print(f'Completed {file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
